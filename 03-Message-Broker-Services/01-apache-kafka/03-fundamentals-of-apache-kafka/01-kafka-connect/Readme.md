# Kafka Connect

## Table of Contents

# Kafka Connect Concepts

## Kafka Connect Concept 1. Connectors

- **Kafka Connect** includes two types of connectors:
  1. **Source connector**: Used to move data from source systems to **Kafka topics**.
  2. **Sink connector**: Used to send data from **Kafka topics** into target (sink) systems.
- Remarks:
  - **Confluent** offers several [pre-built connectors](https://www.confluent.io/product/connectors/?_ga=2.113725121.770498573.1720359767-234518971.1709664712&_gl=1*1owcoes*_gcl_au*OTIxNjA2MDMuMTcxNzYxMTg4NA..*_ga*MjM0NTE4OTcxLjE3MDk2NjQ3MTI.*_ga_D2D3EGKSGD*MTcyMDM1OTc2Ny45NS4xLjE3MjAzNjAyMDAuNTkuMC4w) that can be used to stream data to or from commonly used systems, such as relational databases or HDFS. In order to efficiently discuss the inner workings of Kafka Connect, it is helpful to establish a few major concepts.

### 1.1 Lists Of Kafka Connections

#### 1.1.1. JdbcSinkConnector (`io.confluent.connect.jdbc.JdbcSinkConnector`)

- **Purpose**: This connector is used to ingest data from **Kafka topics** into relational databases using **JDBC** (Java Database Connectivity).
- **Functionality**: It allows you to define configurations to map **Kafka topic** data to database tables, handle data transformations, manage schemas, and configure how to handle errors and retries during data ingestion.
- **Configuration**: Requires JDBC connection properties, table schema, and data mapping configurations.
- **Use Case**: Typically used for streaming data from **Kafka** into traditional relational databases like PostgreSQL, MySQL, Oracle, etc.

#### 1.1.2. BigQuerySinkConnector (`com.wepay.kafka.connect.bigquery.BigQuerySinkConnector`)

- **Purpose**: This connector is specifically designed to stream data from **Kafka topics** into **Google BigQuery**, which is a serverless, highly scalable, and cost-effective multi-cloud data warehouse.
- **Functionality**: It handles tasks such as mapping **Kafka topic** data to **BigQuery** tables, managing schema evolution, specifying BigQuery dataset configurations, handling errors, and integrating with Google Cloud Storage (GCS) for temporary storage of data files before loading into BigQuery.
- **Configuration**: Requires BigQuery credentials, project ID, dataset, table name, and other BigQuery-specific configurations.
- **Use Case**: Ideal for real-time analytics and reporting where data streaming from Kafka needs to be immediately available for analysis in BigQuery.

## Kafka Connect Concept 2. Dead Letter Queue (DLQ)

- **DLQs** are only applicable for **sink connectors**. Note that for **Confluent Cloud sink connectors** a **DLQ topic** is autogenerated.

# Resources and Further Reading

1. 
2. [support.etlworks.com/articles - Real-time-change-replication-with-Kafka-and-Debezium](https://support.etlworks.com/hc/en-us/articles/360020461693-Real-time-change-replication-with-Kafka-and-Debezium)
